{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72b6829e-91ef-4eb6-8184-20b6ae9e9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bea51584-30da-49d7-8b15-91bc7c016d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import ElectraTokenizerFast, ElectraForTokenClassification\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-small-finetuned-naver-ner\")\n",
    "model = ElectraForTokenClassification.from_pretrained(\"monologg/koelectra-small-finetuned-naver-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b47bfb4-072c-4bd7-ace6-3d3fb396ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    BasicTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    Pipeline\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def custom_encode_plus(sentence,\n",
    "                       tokenizer,\n",
    "                       return_tensors=None):\n",
    "    # {'input_ids': [2, 10841, 10966, 10832, 10541, 21509, 27660, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "    words = sentence.split()\n",
    "\n",
    "    tokens = []\n",
    "    tokens_mask = []\n",
    "\n",
    "    for word in words:\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [tokenizer.unk_token]  # For handling the bad-encoded word\n",
    "        tokens.extend(word_tokens)\n",
    "        tokens_mask.extend([1] + [0] * (len(word_tokens) - 1))\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    len_ids = len(ids)\n",
    "    total_len = len_ids + tokenizer.num_special_tokens_to_add()\n",
    "    if tokenizer.model_max_length and total_len > tokenizer.model_max_length:\n",
    "        ids, _, _ = tokenizer.truncate_sequences(\n",
    "            ids,\n",
    "            pair_ids=None,\n",
    "            num_tokens_to_remove=total_len - tokenizer.model_max_length,\n",
    "            truncation_strategy=\"longest_first\",\n",
    "            stride=0,\n",
    "        )\n",
    "\n",
    "    sequence = tokenizer.build_inputs_with_special_tokens(ids)\n",
    "    token_type_ids = tokenizer.create_token_type_ids_from_sequences(ids)\n",
    "    # HARD-CODED: As I know, most of the transformers architecture will be `[CLS] + text + [SEP]``\n",
    "    #             Only way to safely cover all the cases is to integrate `token mask builder` in internal library.\n",
    "    tokens_mask = [1] + tokens_mask + [1]\n",
    "    words = [tokenizer.cls_token] + words + [tokenizer.sep_token]\n",
    "\n",
    "    encoded_inputs = {}\n",
    "    encoded_inputs[\"input_ids\"] = sequence\n",
    "    encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "\n",
    "    encoded_inputs[\"input_ids\"] = torch.tensor([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "    if \"token_type_ids\" in encoded_inputs:\n",
    "        encoded_inputs[\"token_type_ids\"] = torch.tensor([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "    if \"attention_mask\" in encoded_inputs:\n",
    "        encoded_inputs[\"attention_mask\"] = torch.tensor([encoded_inputs[\"attention_mask\"]])\n",
    "\n",
    "    elif return_tensors is not None:\n",
    "        logger.warning(\n",
    "            \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                return_tensors\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return encoded_inputs, words, tokens_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46470fcf-6f56-4667-ae80-0640dc1bb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"2009년 7월 FC서울을 떠나 잉글랜드 프리미어리그 볼턴 원더러스로 이적한 이청용은 크리스탈 팰리스와 독일 분데스리가2 VfL 보훔을 거쳐 지난 3월 K리그로 컴백했다. 행선지는 서울이 아닌 울산이었다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e9d4cb2-8894-4813-85da-874a424a1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, words, tokens_mask = custom_encode_plus(\n",
    "                sentence,\n",
    "                tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f09d1f7e-d76f-4f67-8748-828eb6f37062",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = {name: tensor.to() for name, tensor in tokens.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d36fc-781d-4acd-8046-212cc682acb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882bfdc2-50ff-48b5-8d88-0ca2c93edbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model(**token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e7dc78-54eb-44e4-819d-e0de6a7394f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = entities[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c0ffab-de40-4ab7-a23f-ddb8f68ee72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.exp(result) / np.exp(result).sum(-1, keepdims=True)\n",
    "labels_idx = score.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd81ebf-b80c-4e06-aa32-48d414e1dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_level_answer = []\n",
    "input_ids = tokens[\"input_ids\"].numpy()[0]\n",
    "\n",
    "for idx, label_idx in enumerate(labels_idx):\n",
    "    # NOTE Append every answer even though the `entity` is in `ignore_labels`\n",
    "    token_level_answer += [\n",
    "        {\n",
    "            \"word\": tokenizer.convert_ids_to_tokens(int(input_ids[idx])),\n",
    "            \"score\": score[idx][label_idx].item(),\n",
    "            \"entity\": model.config.id2label[label_idx],\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d5ba3-94de-4ea0-b8da-cc26c03f1b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94306a-e37b-4e3d-9036-b58ee4f77750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8394ad-487e-4857-8515-343d02adeabd",
   "metadata": {},
   "source": [
    "# IPU model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f47160-43bc-46eb-a9f7-31fdf650dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f68bb9e-28d5-4f98-918f-cea8ada5948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_electra import PipelinedElectraForTokenClassification\n",
    "from easydict import EasyDict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "911cb723-ae59-4745-8a0a-1d8f0bae27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'finetune/squad_configurations.yaml'\n",
    "config = EasyDict(yaml.load(open(config_file).read(), Loader=yaml.Loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9c24fc-2a9e-46dc-bd3e-44105dfbf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ipu_config = {\n",
    "    \"layers_per_ipu\": config.train_config.train_layers_per_ipu,\n",
    "    \"recompute_checkpoint_every_layer\": config.train_config.train_recompute_checkpoint_every_layer,\n",
    "    \"embedding_serialization_factor\": config.train_config.train_embedding_serialization_factor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a639f64a-48f2-4e91-90c1-5c7a76f33cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelinedElectraForTokenClassification.from_pretrained_transformers(\"monologg/koelectra-small-finetuned-naver-ner\", train_ipu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf389f72-b353-42e4-ab48-8fbe9825ffcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f3d80d-ae06-4b4c-852e-52b230e7d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "from finetune.run_squad_ipu import ipu_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b91790d9-3108-4de3-b001-5964d3d94363",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_micro_batch_size = 1\n",
    "valid_replication_factor = 1\n",
    "valid_global_batch_size = valid_micro_batch_size * valid_replication_factor\n",
    "valid_device_iterations = 1\n",
    "valid_samples_per_iteration = valid_global_batch_size * valid_device_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190c64c9-c203-481c-8034-1b3fa6dfed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_samples_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f237f996-fbcf-4feb-adea-c8bda9f61d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_opts = ipu_options(1, valid_replication_factor, valid_device_iterations, train_option=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afcd4feb-355d-42d6-a554-c2fb634516a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = poptorch.inferenceModel(model, val_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a1d1dd5-4557-442f-8758-00ddaad0d7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "           6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "           5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "           6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "           5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "          10822,     3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2047807b-32b7-4a0b-aac8-dd3378d187c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.vstack((token['input_ids'],token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86645fa6-12cc-48dd-969d-e11dd25c1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_type_ids = torch.vstack((token['token_type_ids'],token['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f360d82-160b-4224-9d70-7ba7738bfe3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "           6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "           5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "           6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "           5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "          10822,     3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67a0b9e4-f3d4-4ee6-974d-0f09a4f7967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:38<00:00]\n"
     ]
    }
   ],
   "source": [
    "entities = inference_model(token['input_ids'],token['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89144497-3755-45d6-9eef-c93140bd7f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea0df7-3c92-4079-854f-672faabda6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
