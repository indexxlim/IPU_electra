{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b6829e-91ef-4eb6-8184-20b6ae9e9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea51584-30da-49d7-8b15-91bc7c016d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import ElectraTokenizerFast, ElectraForTokenClassification\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-small-finetuned-naver-ner\")\n",
    "model = ElectraForTokenClassification.from_pretrained(\"monologg/koelectra-small-finetuned-naver-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ffe60-f4d3-48af-95ce-299423547430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b47bfb4-072c-4bd7-ace6-3d3fb396ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    BasicTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    Pipeline\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def custom_encode_plus(sentence,\n",
    "                       tokenizer,\n",
    "                       return_tensors=None):\n",
    "    # {'input_ids': [2, 10841, 10966, 10832, 10541, 21509, 27660, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "    words = sentence.split()\n",
    "\n",
    "    tokens = []\n",
    "    tokens_mask = []\n",
    "\n",
    "    for word in words:\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [tokenizer.unk_token]  # For handling the bad-encoded word\n",
    "        tokens.extend(word_tokens)\n",
    "        tokens_mask.extend([1] + [0] * (len(word_tokens) - 1))\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    len_ids = len(ids)\n",
    "    total_len = len_ids + tokenizer.num_special_tokens_to_add()\n",
    "    if tokenizer.model_max_length and total_len > tokenizer.model_max_length:\n",
    "        ids, _, _ = tokenizer.truncate_sequences(\n",
    "            ids,\n",
    "            pair_ids=None,\n",
    "            num_tokens_to_remove=total_len - tokenizer.model_max_length,\n",
    "            truncation_strategy=\"longest_first\",\n",
    "            stride=0,\n",
    "        )\n",
    "\n",
    "    sequence = tokenizer.build_inputs_with_special_tokens(ids)\n",
    "    token_type_ids = tokenizer.create_token_type_ids_from_sequences(ids)\n",
    "    # HARD-CODED: As I know, most of the transformers architecture will be `[CLS] + text + [SEP]``\n",
    "    #             Only way to safely cover all the cases is to integrate `token mask builder` in internal library.\n",
    "    tokens_mask = [1] + tokens_mask + [1]\n",
    "    words = [tokenizer.cls_token] + words + [tokenizer.sep_token]\n",
    "\n",
    "    encoded_inputs = {}\n",
    "    encoded_inputs[\"input_ids\"] = sequence\n",
    "    encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "    encoded_inputs[\"input_ids\"] = torch.tensor([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "    if \"token_type_ids\" in encoded_inputs:\n",
    "        encoded_inputs[\"token_type_ids\"] = torch.tensor([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "    if \"attention_mask\" in encoded_inputs:\n",
    "        encoded_inputs[\"attention_mask\"] = torch.tensor([encoded_inputs[\"attention_mask\"]])\n",
    "\n",
    "    elif return_tensors is not None:\n",
    "        logger.warning(\n",
    "            \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                return_tensors\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return encoded_inputs, words, tokens_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e669c50-37c3-45ea-b7d6-4c515a336776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46470fcf-6f56-4667-ae80-0640dc1bb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"2009년 7월 FC서울을 떠나 잉글랜드 프리미어리그 볼턴 원더러스로 이적한 이청용은 크리스탈 팰리스와 독일 분데스리가2 VfL 보훔을 거쳐 지난 3월 K리그로 컴백했다. 행선지는 서울이 아닌 울산이었다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9d4cb2-8894-4813-85da-874a424a1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, words, tokens_mask = custom_encode_plus(sentence, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a864ee-e5bd-4b85-93bf-5bd168b83361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "           6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "           5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "           6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "           5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "          10822,     3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce6798-c884-4044-97b9-66fada088c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6daa7cec-5885-4596-a85e-bbc1589c880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode_plus(sentence,\n",
    "    padding='longest',\n",
    "    max_length=512,\n",
    "    truncation=True, \n",
    "    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147f7bb-10ff-4a7c-9278-7cc44f111ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83598a-2352-4359-a72f-f52c99fce178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "882bfdc2-50ff-48b5-8d88-0ca2c93edbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e7dc78-54eb-44e4-819d-e0de6a7394f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = entities[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c0ffab-de40-4ab7-a23f-ddb8f68ee72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.exp(result) / np.exp(result).sum(-1, keepdims=True)\n",
    "labels_idx = score.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd81ebf-b80c-4e06-aa32-48d414e1dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_level_answer = []\n",
    "input_ids = tokens[\"input_ids\"].numpy()[0]\n",
    "\n",
    "for idx, label_idx in enumerate(labels_idx):\n",
    "    # NOTE Append every answer even though the `entity` is in `ignore_labels`\n",
    "    token_level_answer += [\n",
    "        {\n",
    "            \"word\": tokenizer.convert_ids_to_tokens(int(input_ids[idx])),\n",
    "            \"score\": score[idx][label_idx].item(),\n",
    "            \"entity\": model.config.id2label[label_idx],\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07e8d3-632c-475b-99d3-d1f7e7f68ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e85e5-d26a-4060-bc31-663630c4ac67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b845847-e184-4dc4-9f3d-cf3e7d04ecd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08784159-2c62-4ceb-8bbf-2beaddd76958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860bc02-892b-44da-bb13-419aa23b7398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc94306a-e37b-4e3d-9036-b58ee4f77750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data = pd.read_csv(data_path, sep='\\t', names = ['text', 'label'])\n",
    "        \n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        for i in range(len(data)):\n",
    "            self.texts.append(data.iloc[i].text.split())\n",
    "            self.labels.append(data.iloc[i].label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'text':self.texts[index], 'label':self.labels[index]}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9920980c-8c4a-44af-b32f-6f63708f5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NERCollator:\n",
    "    def __init__(self, tokenizer, sequence_length=512, mapping=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = 'text'\n",
    "        self.label = 'label'\n",
    "        self.pad_token_label_id = -100\n",
    "\n",
    "        labels_lst = [\"O\",\n",
    "                \"PER-B\", \"PER-I\", \"FLD-B\", \"FLD-I\", \"AFW-B\", \"AFW-I\", \"ORG-B\", \"ORG-I\",\n",
    "                \"LOC-B\", \"LOC-I\", \"CVL-B\", \"CVL-I\", \"DAT-B\", \"DAT-I\", \"TIM-B\", \"TIM-I\",\n",
    "                \"NUM-B\", \"NUM-I\", \"EVT-B\", \"EVT-I\", \"ANM-B\", \"ANM-I\", \"PLT-B\", \"PLT-I\",\n",
    "                \"MAT-B\", \"MAT-I\", \"TRM-B\", \"TRM-I\"]\n",
    "        self.labels_dict = {label:i for i, label in enumerate(labels_lst)}\n",
    "\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.text not in batch[0] or self.label not in batch[0]:\n",
    "            raise Exception(\"Error: Undefined data keys\")\n",
    "\n",
    "        sentence = [item[self.text] for item in batch]\n",
    "\n",
    "        source_batch = self.tokenizer.batch_encode_plus(sentence,\n",
    "                    padding='max_length',\n",
    "                    is_split_into_words=True,\n",
    "                    max_length=512,\n",
    "                    truncation=True, \n",
    "                    return_offsets_mapping=True,\n",
    "                    return_tensors='pt')\n",
    "\n",
    "        labels = [[self.labels_dict[la] for la in item[self.label].split()] for item in batch]\n",
    "        sequence_length = source_batch.input_ids.shape[1]\n",
    "        labels = [label + [self.pad_token_label_id] * (sequence_length-len(label)) for label in labels]\n",
    "        \n",
    "\n",
    "        return {'input_ids':source_batch.input_ids,\n",
    "                 'attention_mask':source_batch.attention_mask,\n",
    "                 'token_type_ids':source_batch.token_type_ids,\n",
    "                 'labels': torch.tensor(labels),\n",
    "                 'offset_mapping': source_batch.offset_mapping,\n",
    "                 'sentence': sentence\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3828803b-815f-486a-b689-5adee97fc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = NERDataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3f1632-138d-4165-9a88-f362cafc3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = NERCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c7dd167-a5ff-4836-bcc7-5588cfd232b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl = DataLoader(dataset, batch_size=4, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1804b-aae4-4454-9404-ffd7ec4d4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d497840-bc64-441c-962e-052fbc3949e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ElectraTokenizer, ElectraForTokenClassification\n",
    "# import torch\n",
    "\n",
    "# tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "# model = ElectraForTokenClassification.from_pretrained('google/electra-small-discriminator')\n",
    "\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# loss, scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7129c34-1768-49f4-ba48-d7282b755f64",
   "metadata": {},
   "source": [
    "# IPU model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1e2d03-5afe-4a55-ae6b-68453c3897bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning-Electra.ipynb  evaluate_v1_0.py\t     ner_configurations.yaml\n",
      "__init__.py\t\t   exe_cache\t\t     run_korquad_ipu.py\n",
      "__pycache__\t\t   koelectra_korquad.ipynb   run_ner_ipu.py\n",
      "checkpoints\t\t   korquad_preprocessing.py  squad_configurations.yaml\n",
      "dataloader_ner.py\t   naver_ner.ipynb\t     squad_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10745535-0d89-4454-b7d0-7df4024659d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e2f0c8-465f-4500-88a0-fdfe4a8e3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "import yaml\n",
    "from pipeline_electra import PipelinedElectraForTokenClassification\n",
    "import poptorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2408b423-7a90-4c5e-a301-c609223d31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"finetune/ner_configurations.yaml\"\n",
    "config = EasyDict(yaml.load(open(config_file).read(), Loader=yaml.Loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "798c844e-cbc1-490f-bdbc-6b4af8ee485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NERDataset(config.train_data_path)\n",
    "collator = NERCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd989191-9ef7-49cd-b7ae-74097ab81004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size=4, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ea6283a-bfcb-4b3e-93d4-cc833481fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570a43a-f365-4978-b200-2457483d4a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5a8da-21b3-434a-b26a-d372a4557b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fa3b4a1-6bd7-4fe6-9682-7988bed0ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing PipelinedElectraForTokenClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing PipelinedElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PipelinedElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PipelinedElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#data_path\n",
    "train_ipu_config = {\n",
    "    \"layer_per_ipu\": config.train_config.train_layers_per_ipu,\n",
    "    \"recompute_checkpoint_every_layer\": config.train_config.train_recompute_checkpoint_every_layer,\n",
    "    \"embedding_serialization_factor\": config.train_config.train_embedding_serialization_factor\n",
    "}\n",
    "\n",
    "train_ipu_config = EasyDict(train_ipu_config)\n",
    "model = PipelinedElectraForTokenClassification.from_pretrained_transformers(config.train_config.model_name_or_path, train_ipu_config, num_labels=29)\n",
    "\n",
    "# model.parallelize().half().train()\n",
    "\n",
    "train_global_batch_size = config.train_config.train_global_batch_size\n",
    "train_micro_batch_size = config.train_config.train_micro_batch_size\n",
    "train_replication_factor = config.train_config.train_replication_factor\n",
    "gradient_accumulation = int(train_global_batch_size / train_micro_batch_size / train_replication_factor)\n",
    "train_device_iterations = config.train_config.train_device_iterations\n",
    "train_samples_per_iteration = train_global_batch_size * train_device_iterations\n",
    "num_epochs = config.train_config.num_epochs\n",
    "\n",
    "from finetune.run_ner_ipu import ipu_options\n",
    "train_opts = ipu_options(gradient_accumulation, train_replication_factor, train_device_iterations, train_option=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44d41ab5-b3d6-4815-9f15-c17ea19812c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_params = []\n",
    "non_regularized_params = []\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        if len(param.shape) == 1:\n",
    "            non_regularized_params.append(param)\n",
    "        else:\n",
    "            regularized_params.append(param)\n",
    "\n",
    "params = [\n",
    "    {\"params\": regularized_params, \"weight_decay\": 0.01},\n",
    "    {\"params\": non_regularized_params, \"weight_decay\": 0}\n",
    "]\n",
    "optimizer = poptorch.optim.AdamW(params,\n",
    "                                 lr=1e-4,\n",
    "                                 weight_decay=0,\n",
    "                                 eps=1e-6,\n",
    "                                 bias_correction=True,\n",
    "                                 loss_scaling=64,\n",
    "                                 first_order_momentum_accum_type=torch.float16,\n",
    "                                 accum_type=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99174b7e-fb3b-4c4c-a3a7-747944da33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model = poptorch.trainingModel(model, train_opts, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03407462-b6fe-48c8-8084-fad59063538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a653a76e-54e8-4bf1-b1dd-2cda0d668b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nerdataset = NERDataset(config.train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55de6c96-7c70-4559-915c-808d8a7f82dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07:35:13.547] [poptorch::python] [warning] The number of elements in the dataset (73945) is not divisible by the number of elements processed per step (128) and drop_last=False. The last tensor will have a batch size of 89. To avoid having to handle this special case switch to drop_last=True\n"
     ]
    }
   ],
   "source": [
    "train_dl = poptorch.DataLoader(train_opts,\n",
    "                               nerdataset,\n",
    "                               batch_size=train_micro_batch_size,\n",
    "                               collate_fn=collator,\n",
    "                               shuffle=True,\n",
    "                               drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8647521-e5d4-4f6e-9cfb-95fa67b5b222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f8ca1-c64b-466c-90e1-d62526f2ff8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3b00d2b-7df6-490e-989d-1f8b8e9decc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a42f517b-91d6-4b23-9c94-58bea871948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_one = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1515a19a-c6f7-43f1-9aa0-c123c383da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = transformers.get_scheduler(\"cosine\", optimizer, 0.1 * num_steps, num_steps)\n",
    "\n",
    "# Wrap the pytorch model with poptorch.trainingModel\n",
    "training_model = poptorch.trainingModel(model, train_opts, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6bec7-69e2-47d4-900a-796b94639e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eca48f-3fb6-486b-9a00-16ed5efe3d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb60c51-a129-4afa-bfc0-85465371e575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17a9202e-1e77-4079-a107-ed8395ab4168",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "In poptorch/source/RemoveSurplusIdentityLosses.cpp:103: 'poptorch_cpp_error': Couldn't find a loss in graph!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f497cf8fb5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                 batch[\"labels\"])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/poptorch/_poplar_executor.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m             )\n\u001b[1;32m    621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloadExecutable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/poptorch/_poplar_executor.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(self, in_tensors)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                     self._executable = poptorch_core.compileWithTrace(\n\u001b[0;32m--> 505\u001b[0;31m                         *trace_args)\n\u001b[0m\u001b[1;32m    506\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compileWithDispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: In poptorch/source/RemoveSurplusIdentityLosses.cpp:103: 'poptorch_cpp_error': Couldn't find a loss in graph!"
     ]
    }
   ],
   "source": [
    "# Compile model or load from executable cache\n",
    "batch = next(iter(train_dl))\n",
    "outputs = training_model.compile(batch[\"input_ids\"],\n",
    "                                 batch[\"attention_mask\"],\n",
    "                                 batch[\"token_type_ids\"],\n",
    "                                batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59608c8-8353-4ab0-af9a-32182afd212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb419f49-f392-4bbc-a1a2-d1605b5c4045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d7054-2370-40b6-939b-ef1000a8909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "    train_iter = tqdm(train_dl)\n",
    "    for step, batch in enumerate(train_iter):\n",
    "        start_step = time.perf_counter()\n",
    "\n",
    "        # This completes a forward+backward+weight update step\n",
    "        outputs = training_model(batch[\"input_ids\"],\n",
    "                                 batch[\"attention_mask\"],\n",
    "                                 batch[\"token_type_ids\"])\n",
    "\n",
    "        # Update the LR and update the poptorch optimizer\n",
    "        lr_scheduler.step()\n",
    "        training_model.setOptimizer(optimizer)\n",
    "        step_length = time.perf_counter() - start_step\n",
    "        step_throughput = samples_per_iteration / step_length\n",
    "        loss = outputs[0].mean().item()\n",
    "        train_iter.set_description(\n",
    "            f\"Epoch: {epoch} - \"\n",
    "            f\"Step: {step} - \"\n",
    "            f\"Loss: {loss:3.3f} - \"\n",
    "            f\"Throughput: {step_throughput:3.3f} seq/s\")\n",
    "\n",
    "# Detach the model from the device once training is over so the device is free to be reused for validation\n",
    "training_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb643239-fe8c-401b-b086-fd4f5074a9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03b2f3-a050-4b48-ad6c-85ea1872f7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8394ad-487e-4857-8515-343d02adeabd",
   "metadata": {},
   "source": [
    "# IPU model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59f47160-43bc-46eb-a9f7-31fdf650dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f68bb9e-28d5-4f98-918f-cea8ada5948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_electra import PipelinedElectraForTokenClassification\n",
    "from easydict import EasyDict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "911cb723-ae59-4745-8a0a-1d8f0bae27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'finetune/squad_configurations.yaml'\n",
    "config = EasyDict(yaml.load(open(config_file).read(), Loader=yaml.Loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cf9c24fc-2a9e-46dc-bd3e-44105dfbf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ipu_config = {\n",
    "    \"layers_per_ipu\": config.train_config.train_layers_per_ipu,\n",
    "    \"recompute_checkpoint_every_layer\": config.train_config.train_recompute_checkpoint_every_layer,\n",
    "    \"embedding_serialization_factor\": config.train_config.train_embedding_serialization_factor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a639f64a-48f2-4e91-90c1-5c7a76f33cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelinedElectraForTokenClassification.from_pretrained_transformers(\"monologg/koelectra-small-finetuned-naver-ner\", train_ipu_config, num_labels=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafafce-83d0-4b65-a3d2-b0a4f68cafb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043a8b9-414c-47cc-8568-bbcded21e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afd0d3-1bc2-437e-8906-c1fc3c307d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260ba7be-4add-4710-8903-6130147e9c35",
   "metadata": {},
   "source": [
    "# BatchTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d9e3d7d8-9ae0-409d-92dd-a59c8c65a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'], torch.tensor(batch['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af463ade-bdb8-4b92-b319-e3dac45a9d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c228744-bbad-4cbc-8a94-fde1fc09ccd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4493d-febf-454c-acae-42b5da0ad4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b5f3d80d-ae06-4b4c-852e-52b230e7d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "from finetune.run_squad_ipu import ipu_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b91790d9-3108-4de3-b001-5964d3d94363",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_micro_batch_size = 1\n",
    "valid_replication_factor = 1\n",
    "valid_global_batch_size = valid_micro_batch_size * valid_replication_factor\n",
    "valid_device_iterations = 1\n",
    "valid_samples_per_iteration = valid_global_batch_size * valid_device_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "190c64c9-c203-481c-8034-1b3fa6dfed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_samples_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f237f996-fbcf-4feb-adea-c8bda9f61d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_opts = ipu_options(1, valid_replication_factor, valid_device_iterations, train_option=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "afcd4feb-355d-42d6-a554-c2fb634516a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = poptorch.inferenceModel(model, val_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a1d1dd5-4557-442f-8758-00ddaad0d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token = tokenizer.encode_plus(sentence, return_tensors='pt', return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618038e3-81ef-445b-a543-1b1691119287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f721b3-81ee-4ef6-8ae7-b95df800e2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3f3e629d-4234-49e5-aac0-3efcac89c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sentence.split()\n",
    "split_tokens = tokenizer.encode_plus(word_list, is_split_into_words=True, return_tensors='pt', return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1f1a4a2a-29c1-4f7c-8efb-ac598b7e2ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_offset_mapping = split_tokens['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31809ebd-ca9e-4d2e-b41f-fede0b1e02d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2047807b-32b7-4a0b-aac8-dd3378d187c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.vstack((token['input_ids'],token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "86645fa6-12cc-48dd-969d-e11dd25c1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_type_ids = torch.vstack((token['token_type_ids'],token['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "85063c74-4d41-44b3-a43f-2d3a9afa632d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "          6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "          5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "          6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "          5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "         10822,     3]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "67a0b9e4-f3d4-4ee6-974d-0f09a4f7967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\n"
     ]
    }
   ],
   "source": [
    "entities = inference_model(split_tokens['input_ids'], split_tokens['token_type_ids'])#, split_tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfea48-d3f7-4e97-80b9-caf66fe7869b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db7f811-0fac-4f5b-9928-8fc69a754748",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_offset_mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8429c2aad811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mignore_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mword_offset_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_offset_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_offset_mapping' is not defined"
     ]
    }
   ],
   "source": [
    "ignore_labels=[\"O\"]\n",
    "ignore_special_tokens=True\n",
    "\n",
    "word_offset_mapping = word_offset_mapping[0].cpu().numpy()\n",
    "entities = entities[0].cpu().numpy()\n",
    "input_ids = split_tokens[\"input_ids\"].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dad817-d21c-43f3-a6a1-6f184b723b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14b8c8-4f48-4c09-ac98-523c5179e351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "39ea0df7-3c92-4079-854f-672faabda6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n",
    "labels_idx = score.argmax(axis=-1)\n",
    "\n",
    "token_level_answer = []\n",
    "for idx, label_idx in enumerate(labels_idx):\n",
    "    if model.config.id2label[label_idx] in ignore_labels:\n",
    "        token_answer = []\n",
    "        continue\n",
    "    \n",
    "    elif word_offset_mapping[idx][0] == 0:\n",
    "        if token_answer:\n",
    "            token_answer[\"word\"] = tokenizer.decode(token_answer[\"word\"])\n",
    "            token_level_answer.append(token_answer)\n",
    "        token_answer = {\n",
    "                \"word\": [int(input_ids[idx])],\n",
    "                \"score\": score[idx][label_idx].item(),\n",
    "                \"entity\": model.config.id2label[label_idx],\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        token_answer['word'].append(int(input_ids[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13daad8-415c-4413-9adf-cb2ce96fd323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4d6f475f-7df4-48aa-8d27-40e7bf957076",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i, word_offset in enumerate(word_offset_mapping):\n",
    "    if (word_offset[1] - word_offset[0]) == 0:\n",
    "        token = []\n",
    "        continue\n",
    "    elif word_offset[0] == 0:\n",
    "        if token:\n",
    "            tokens.append(token)\n",
    "        token = []\n",
    "        token.append(split_tokens['input_ids'][0][i])\n",
    "    else:\n",
    "        token.append(split_tokens['input_ids'][0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "12a3a2ec-8bb6-48fd-b37b-f5f3c1ff847c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 5],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [2, 4],\n",
       "       [4, 5],\n",
       "       [0, 2],\n",
       "       [0, 4],\n",
       "       [0, 6],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 5],\n",
       "       [0, 2],\n",
       "       [2, 3],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [0, 4],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 4],\n",
       "       [0, 2],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 5],\n",
       "       [5, 6],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [3, 4],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [4, 5],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [0, 2],\n",
       "       [2, 3],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [2, 5],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "29fd9399-8afe-4d1e-a3c2-bd7c5f6c5226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009년'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a57cc14c-bf7a-4139-b45d-2fc851f9da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009년\n",
      "7월\n",
      "FC서울을\n",
      "떠나\n",
      "잉글랜드\n",
      "프리미어리그\n",
      "볼턴\n",
      "원더러스로\n",
      "이적한\n",
      "이청용은\n",
      "크리스탈\n",
      "팰리스와\n",
      "독일\n",
      "분데스리가2\n",
      "VfL\n",
      "보훔을\n",
      "거쳐\n",
      "지난\n",
      "3월\n",
      "K리그로\n",
      "컴백했다.\n",
      "행선지는\n",
      "서울이\n",
      "아닌\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80acf3-f702-4c43-b4e2-73f88707938c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaa82c-46de-40e3-84d4-7df45f8fdea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26757fb4-3f73-4c8f-b4b6-408967e48d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cur_offset_mapping = offset_mapping[0]\n",
    "for i, offset in enumerate(cur_offset_mapping[1:-1]):\n",
    "    #print(offset)\n",
    "    if (cur_offset_mapping[i+1][0] - offset[1]) > 0:\n",
    "        continue\n",
    "    else:\n",
    "        print(token_level_answer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4037bf49-7b77-4558-944f-0c157ce274ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '7월', 'score': 0.9350952506065369, 'entity': 'DAT-I'},\n",
       " {'word': 'FC', 'score': 0.9994584321975708, 'entity': 'ORG-B'},\n",
       " {'word': '##서울', 'score': 0.9809041023254395, 'entity': 'ORG-I'},\n",
       " {'word': '##을', 'score': 0.9081450700759888, 'entity': 'O'},\n",
       " {'word': '떠나', 'score': 0.9999575614929199, 'entity': 'O'},\n",
       " {'word': '잉글랜드', 'score': 0.9983227849006653, 'entity': 'LOC-B'},\n",
       " {'word': '프리미어리그', 'score': 0.9989780187606812, 'entity': 'ORG-B'},\n",
       " {'word': '볼', 'score': 0.9300729632377625, 'entity': 'ORG-B'},\n",
       " {'word': '##턴', 'score': 0.9710206389427185, 'entity': 'ORG-I'},\n",
       " {'word': '원', 'score': 0.999349057674408, 'entity': 'ORG-I'},\n",
       " {'word': '##더러', 'score': 0.9925868511199951, 'entity': 'ORG-I'},\n",
       " {'word': '##스로', 'score': 0.9959636926651001, 'entity': 'ORG-I'},\n",
       " {'word': '이적', 'score': 0.9999213814735413, 'entity': 'O'},\n",
       " {'word': '##한', 'score': 0.9999334216117859, 'entity': 'O'},\n",
       " {'word': '이', 'score': 0.9994910359382629, 'entity': 'PER-B'},\n",
       " {'word': '##청', 'score': 0.7056900262832642, 'entity': 'PER-B'},\n",
       " {'word': '##용은', 'score': 0.5724880695343018, 'entity': 'PER-B'},\n",
       " {'word': '크리스탈', 'score': 0.9994640350341797, 'entity': 'ORG-B'},\n",
       " {'word': '팰', 'score': 0.9991778135299683, 'entity': 'ORG-I'},\n",
       " {'word': '##리스', 'score': 0.9978461265563965, 'entity': 'ORG-I'},\n",
       " {'word': '##와', 'score': 0.9522739052772522, 'entity': 'O'},\n",
       " {'word': '독일', 'score': 0.9977171421051025, 'entity': 'LOC-B'},\n",
       " {'word': '분', 'score': 0.9814788699150085, 'entity': 'ORG-B'},\n",
       " {'word': '##데스', 'score': 0.8572928309440613, 'entity': 'ORG-B'},\n",
       " {'word': '##리가', 'score': 0.9942399859428406, 'entity': 'ORG-I'},\n",
       " {'word': '##2', 'score': 0.9120205640792847, 'entity': 'ORG-I'},\n",
       " {'word': 'V', 'score': 0.8719519376754761, 'entity': 'ORG-B'},\n",
       " {'word': '##f', 'score': 0.7203494310379028, 'entity': 'ORG-I'},\n",
       " {'word': '##L', 'score': 0.9656755328178406, 'entity': 'ORG-I'},\n",
       " {'word': '보', 'score': 0.9938763380050659, 'entity': 'ORG-I'},\n",
       " {'word': '##훔', 'score': 0.9126798510551453, 'entity': 'ORG-I'},\n",
       " {'word': '##을', 'score': 0.6827937960624695, 'entity': 'O'},\n",
       " {'word': '거쳐', 'score': 0.9999727606773376, 'entity': 'O'},\n",
       " {'word': '지난', 'score': 0.9963383078575134, 'entity': 'DAT-B'},\n",
       " {'word': '3월', 'score': 0.9909418225288391, 'entity': 'DAT-I'},\n",
       " {'word': 'K리그', 'score': 0.9995406270027161, 'entity': 'ORG-B'},\n",
       " {'word': '##로', 'score': 0.9990398287773132, 'entity': 'O'},\n",
       " {'word': '컴', 'score': 0.9999098181724548, 'entity': 'O'},\n",
       " {'word': '##백', 'score': 0.998866856098175, 'entity': 'O'},\n",
       " {'word': '##했다', 'score': 0.9999646544456482, 'entity': 'O'},\n",
       " {'word': '.', 'score': 0.9999403357505798, 'entity': 'O'},\n",
       " {'word': '행', 'score': 0.9993055462837219, 'entity': 'O'},\n",
       " {'word': '##선', 'score': 0.9969778656959534, 'entity': 'O'},\n",
       " {'word': '##지는', 'score': 0.9959809184074402, 'entity': 'O'},\n",
       " {'word': '서울', 'score': 0.9918311238288879, 'entity': 'ORG-B'},\n",
       " {'word': '##이', 'score': 0.9753754138946533, 'entity': 'O'},\n",
       " {'word': '아닌', 'score': 0.9999187588691711, 'entity': 'O'},\n",
       " {'word': '울산', 'score': 0.9994460940361023, 'entity': 'ORG-B'}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_level_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e4558a63-be2d-4619-affb-ca5f25405071",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_labels=[\"O\"]\n",
    "ignore_special_tokens=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d6b55a34-e27d-4e85-8b77-729e521860aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [FIX] Now let's change it to word-level NER\n",
    "word_idx = 0\n",
    "word_level_answer = []\n",
    "\n",
    "# NOTE: Might not be safe. BERT, ELECTRA etc. won't make issues.\n",
    "if ignore_special_tokens:\n",
    "    words = words[1:-1]\n",
    "    tokens_mask = tokens_mask[1:-1]\n",
    "    token_level_answer = token_level_answer[1:-1]\n",
    "\n",
    "for mask, ans in zip(tokens_mask, token_level_answer):\n",
    "    if mask == 1:\n",
    "        ans[\"word\"] = words[word_idx]\n",
    "        word_idx += 1\n",
    "        if ans[\"entity\"] not in ignore_labels:\n",
    "            word_level_answer.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73eea1-5ad0-4fe4-80e7-f43ba82399c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39f1ce-ddc2-4214-a29a-fce7a341addc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37fca02-250f-44a1-b9e1-570e8320c542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844f611-b246-4b09-8b75-4440e2c4f327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b445bb-411e-4999-a7b1-125736b2f3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
