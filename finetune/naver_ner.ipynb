{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b6829e-91ef-4eb6-8184-20b6ae9e9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea51584-30da-49d7-8b15-91bc7c016d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import ElectraTokenizerFast, ElectraForTokenClassification\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-small-finetuned-naver-ner\")\n",
    "model = ElectraForTokenClassification.from_pretrained(\"monologg/koelectra-small-finetuned-naver-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ffe60-f4d3-48af-95ce-299423547430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a4ead-473f-4c21-87e3-ddb0e3c3fdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b47bfb4-072c-4bd7-ace6-3d3fb396ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    BasicTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    Pipeline\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def custom_encode_plus(sentence,\n",
    "                       tokenizer,\n",
    "                       return_tensors=None):\n",
    "    # {'input_ids': [2, 10841, 10966, 10832, 10541, 21509, 27660, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "    words = sentence.split()\n",
    "\n",
    "    tokens = []\n",
    "    tokens_mask = []\n",
    "\n",
    "    for word in words:\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [tokenizer.unk_token]  # For handling the bad-encoded word\n",
    "        tokens.extend(word_tokens)\n",
    "        tokens_mask.extend([1] + [0] * (len(word_tokens) - 1))\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    len_ids = len(ids)\n",
    "    total_len = len_ids + tokenizer.num_special_tokens_to_add()\n",
    "    if tokenizer.model_max_length and total_len > tokenizer.model_max_length:\n",
    "        ids, _, _ = tokenizer.truncate_sequences(\n",
    "            ids,\n",
    "            pair_ids=None,\n",
    "            num_tokens_to_remove=total_len - tokenizer.model_max_length,\n",
    "            truncation_strategy=\"longest_first\",\n",
    "            stride=0,\n",
    "        )\n",
    "\n",
    "    sequence = tokenizer.build_inputs_with_special_tokens(ids)\n",
    "    token_type_ids = tokenizer.create_token_type_ids_from_sequences(ids)\n",
    "    # HARD-CODED: As I know, most of the transformers architecture will be `[CLS] + text + [SEP]``\n",
    "    #             Only way to safely cover all the cases is to integrate `token mask builder` in internal library.\n",
    "    tokens_mask = [1] + tokens_mask + [1]\n",
    "    words = [tokenizer.cls_token] + words + [tokenizer.sep_token]\n",
    "\n",
    "    encoded_inputs = {}\n",
    "    encoded_inputs[\"input_ids\"] = sequence\n",
    "    encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "\n",
    "    encoded_inputs[\"input_ids\"] = torch.tensor([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "    if \"token_type_ids\" in encoded_inputs:\n",
    "        encoded_inputs[\"token_type_ids\"] = torch.tensor([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "    if \"attention_mask\" in encoded_inputs:\n",
    "        encoded_inputs[\"attention_mask\"] = torch.tensor([encoded_inputs[\"attention_mask\"]])\n",
    "\n",
    "    elif return_tensors is not None:\n",
    "        logger.warning(\n",
    "            \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                return_tensors\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return encoded_inputs, words, tokens_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46470fcf-6f56-4667-ae80-0640dc1bb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"2009년 7월 FC서울을 떠나 잉글랜드 프리미어리그 볼턴 원더러스로 이적한 이청용은 크리스탈 팰리스와 독일 분데스리가2 VfL 보훔을 거쳐 지난 3월 K리그로 컴백했다. 행선지는 서울이 아닌 울산이었다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e9d4cb2-8894-4813-85da-874a424a1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, words, tokens_mask = custom_encode_plus(sentence, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16a864ee-e5bd-4b85-93bf-5bd168b83361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "          6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "          5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "          6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "          5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "         10822,     3]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee067c3-5ef5-43ab-be56-46f2b15f3c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a504e5-40e2-4c62-81cd-a161e9284cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6daa7cec-5885-4596-a85e-bbc1589c880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "          6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "          5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "          6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "          5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "         10822,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(sentence,\n",
    "    padding='longest',\n",
    "    max_length=512,\n",
    "    truncation=True, \n",
    "    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147f7bb-10ff-4a7c-9278-7cc44f111ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83598a-2352-4359-a72f-f52c99fce178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebb0e8-7fdc-4cfa-b7da-049dc380c62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882bfdc2-50ff-48b5-8d88-0ca2c93edbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model(**token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e7dc78-54eb-44e4-819d-e0de6a7394f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = entities[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c0ffab-de40-4ab7-a23f-ddb8f68ee72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.exp(result) / np.exp(result).sum(-1, keepdims=True)\n",
    "labels_idx = score.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd81ebf-b80c-4e06-aa32-48d414e1dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_level_answer = []\n",
    "input_ids = tokens[\"input_ids\"].numpy()[0]\n",
    "\n",
    "for idx, label_idx in enumerate(labels_idx):\n",
    "    # NOTE Append every answer even though the `entity` is in `ignore_labels`\n",
    "    token_level_answer += [\n",
    "        {\n",
    "            \"word\": tokenizer.convert_ids_to_tokens(int(input_ids[idx])),\n",
    "            \"score\": score[idx][label_idx].item(),\n",
    "            \"entity\": model.config.id2label[label_idx],\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d5ba3-94de-4ea0-b8da-cc26c03f1b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b845847-e184-4dc4-9f3d-cf3e7d04ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f083b260-6e8f-4238-b413-44358c61a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/naver-ner/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3ad1d4c-69e4-40bf-9d04-c5d29bda686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, sep='\\t', names = ['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03646c58-d03b-406b-bf09-b80b7221402f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56575a-cab8-48e9-afa3-ce180f880e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08784159-2c62-4ceb-8bbf-2beaddd76958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a75d19-15f0-45b5-a4dc-a939271aad6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1381f-b6dd-454e-b21a-4eb396672022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dc94306a-e37b-4e3d-9036-b58ee4f77750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data = pd.read_csv(data_path, sep='\\t', names = ['text', 'label'])\n",
    "        \n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        for i in range(len(data)):\n",
    "            self.texts.append(data.iloc[i].text)\n",
    "            self.labels = self.labels2id(data.iloc[i].label.split())\n",
    "\n",
    "                \n",
    "    def labels2id(self, labels):\n",
    "        labels_lst = [\"O\",\n",
    "                \"PER-B\", \"PER-I\", \"FLD-B\", \"FLD-I\", \"AFW-B\", \"AFW-I\", \"ORG-B\", \"ORG-I\",\n",
    "                \"LOC-B\", \"LOC-I\", \"CVL-B\", \"CVL-I\", \"DAT-B\", \"DAT-I\", \"TIM-B\", \"TIM-I\",\n",
    "                \"NUM-B\", \"NUM-I\", \"EVT-B\", \"EVT-I\", \"ANM-B\", \"ANM-I\", \"PLT-B\", \"PLT-I\",\n",
    "                \"MAT-B\", \"MAT-I\", \"TRM-B\", \"TRM-I\"]\n",
    "        labels_dict = {label:i for i, label in enumerate(labels_lst)}\n",
    "        id_value=[]\n",
    "        try:\n",
    "            for label in labels:\n",
    "                id_value.append(labels_dict[label])\n",
    "                print(id_value)\n",
    "        except:\n",
    "            raise Exception(f'Not in NER labels : {label}')\n",
    "        return id_value\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'text':self.texts[index], 'label':self.labels[index]}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9920980c-8c4a-44af-b32f-6f63708f5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NERCollator:\n",
    "    def __init__(self, tokenizer, mapping=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = 'text'\n",
    "        self.label = 'label'    \n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.text not in batch[0] or self.label not in batch[0]:\n",
    "            raise Exception(\"Error: Undefined data keys\")\n",
    "\n",
    "        sentence = [item[self.text]+self.tokenizer.eos_token for item in batch]\n",
    "\n",
    "        source_batch = self.tokenizer.batch_encode_plus(sentence,\n",
    "                    padding='longest',\n",
    "                    max_length=512,\n",
    "                    truncation=True, \n",
    "                    return_tensors='pt')\n",
    "\n",
    "\n",
    "        labels = target_batch.input_ids.clone()\n",
    "        \n",
    "\n",
    "            \n",
    "        return {'input_ids':source_batch.input_ids,\n",
    "                 'attention_mask':source_batch.attention_mask,\n",
    "                 'token_type_ids':source_batch.token_type_ids,\n",
    "                 'labels': labels,\n",
    "                 }\n",
    "    \n",
    "    def custom_encode_plus(self, sentence, tokenizer, return_tensors=None):\n",
    "\n",
    "        words = sentence.split()\n",
    "\n",
    "        tokens = []\n",
    "        tokens_mask = []\n",
    "\n",
    "        for word in words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [tokenizer.unk_token]  # For handling the bad-encoded word\n",
    "            tokens.extend(word_tokens)\n",
    "            tokens_mask.extend([1] + [0] * (len(word_tokens) - 1))\n",
    "\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        len_ids = len(ids)\n",
    "        total_len = len_ids + tokenizer.num_special_tokens_to_add()\n",
    "        if tokenizer.model_max_length and total_len > tokenizer.model_max_length:\n",
    "            ids, _, _ = tokenizer.truncate_sequences(\n",
    "                ids,\n",
    "                pair_ids=None,\n",
    "                num_tokens_to_remove=total_len - tokenizer.model_max_length,\n",
    "                truncation_strategy=\"longest_first\",\n",
    "                stride=0,\n",
    "            )\n",
    "\n",
    "        sequence = tokenizer.build_inputs_with_special_tokens(ids)\n",
    "        token_type_ids = tokenizer.create_token_type_ids_from_sequences(ids)\n",
    "        # HARD-CODED: As I know, most of the transformers architecture will be `[CLS] + text + [SEP]``\n",
    "        #             Only way to safely cover all the cases is to integrate `token mask builder` in internal library.\n",
    "        tokens_mask = [1] + tokens_mask + [1]\n",
    "        words = [tokenizer.cls_token] + words + [tokenizer.sep_token]\n",
    "\n",
    "        encoded_inputs = {}\n",
    "        encoded_inputs[\"input_ids\"] = sequence\n",
    "        encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "\n",
    "        encoded_inputs[\"input_ids\"] = torch.tensor([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "        if \"token_type_ids\" in encoded_inputs:\n",
    "            encoded_inputs[\"token_type_ids\"] = torch.tensor([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "        if \"attention_mask\" in encoded_inputs:\n",
    "            encoded_inputs[\"attention_mask\"] = torch.tensor([encoded_inputs[\"attention_mask\"]])\n",
    "\n",
    "        elif return_tensors is not None:\n",
    "            logger.warning(\n",
    "                \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                    return_tensors\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return encoded_inputs, words, tokens_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb287c1e-e650-42d4-b0da-3ac8626ce9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3828803b-815f-486a-b689-5adee97fc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2id(self, labels):\n",
    "    labels_lst = [\"O\",\n",
    "            \"PER-B\", \"PER-I\", \"FLD-B\", \"FLD-I\", \"AFW-B\", \"AFW-I\", \"ORG-B\", \"ORG-I\",\n",
    "            \"LOC-B\", \"LOC-I\", \"CVL-B\", \"CVL-I\", \"DAT-B\", \"DAT-I\", \"TIM-B\", \"TIM-I\",\n",
    "            \"NUM-B\", \"NUM-I\", \"EVT-B\", \"EVT-I\", \"ANM-B\", \"ANM-I\", \"PLT-B\", \"PLT-I\",\n",
    "            \"MAT-B\", \"MAT-I\", \"TRM-B\", \"TRM-I\"]\n",
    "    labels_dict = {label:i for i, label in enumerate(labels_lst)}\n",
    "    id_value=[]\n",
    "    try:\n",
    "        for label in labels:\n",
    "            id_value.append(labels_dict[label])\n",
    "            print(id_value)\n",
    "    except:\n",
    "        raise Exception(f'Not in NER labels : {label}')\n",
    "    return id_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b619850c-d119-43d6-8a8e-ddff5aca762b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "labels2id() missing 1 required positional argument: 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-81b7c481a925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: labels2id() missing 1 required positional argument: 'labels'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(data_path, sep='\\t', names = ['text', 'label'])\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for i in range(len(data)):\n",
    "    texts.append(data.iloc[i].text)\n",
    "    labels = labels2id(data.iloc[i].label.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c3c4838e-16e5-4efe-b9dc-cc0fe0db8be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'NUM-B', 'NUM-I', 'O', 'O', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[i].label.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd0fc1-4891-4afd-a4a3-c159b5fc2a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a805248-f4a7-4b1a-bed0-48d8eb1b800c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b87a3-94f9-43a1-bc36-5ff7c8704bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af769a9c-e120-44d3-ae13-56c9ae556359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d497840-bc64-441c-962e-052fbc3949e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForTokenClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "model = ElectraForTokenClassification.from_pretrained('google/electra-small-discriminator')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss, scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98ea6a1c-c7ec-4f8c-9389-94e663d686cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09527f7-1277-4814-a02a-edd49c430d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b0d15-0a80-4f7e-8aea-1834c336be31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8394ad-487e-4857-8515-343d02adeabd",
   "metadata": {},
   "source": [
    "# IPU model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f47160-43bc-46eb-a9f7-31fdf650dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f68bb9e-28d5-4f98-918f-cea8ada5948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_electra import PipelinedElectraForTokenClassification\n",
    "from easydict import EasyDict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911cb723-ae59-4745-8a0a-1d8f0bae27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'finetune/squad_configurations.yaml'\n",
    "config = EasyDict(yaml.load(open(config_file).read(), Loader=yaml.Loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9c24fc-2a9e-46dc-bd3e-44105dfbf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ipu_config = {\n",
    "    \"layers_per_ipu\": config.train_config.train_layers_per_ipu,\n",
    "    \"recompute_checkpoint_every_layer\": config.train_config.train_recompute_checkpoint_every_layer,\n",
    "    \"embedding_serialization_factor\": config.train_config.train_embedding_serialization_factor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a639f64a-48f2-4e91-90c1-5c7a76f33cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelinedElectraForTokenClassification.from_pretrained_transformers(\"monologg/koelectra-small-finetuned-naver-ner\", train_ipu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf389f72-b353-42e4-ab48-8fbe9825ffcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5f3d80d-ae06-4b4c-852e-52b230e7d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "from finetune.run_squad_ipu import ipu_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b91790d9-3108-4de3-b001-5964d3d94363",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_micro_batch_size = 1\n",
    "valid_replication_factor = 1\n",
    "valid_global_batch_size = valid_micro_batch_size * valid_replication_factor\n",
    "valid_device_iterations = 1\n",
    "valid_samples_per_iteration = valid_global_batch_size * valid_device_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "190c64c9-c203-481c-8034-1b3fa6dfed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_samples_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f237f996-fbcf-4feb-adea-c8bda9f61d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_opts = ipu_options(1, valid_replication_factor, valid_device_iterations, train_option=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afcd4feb-355d-42d6-a554-c2fb634516a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = poptorch.inferenceModel(model, val_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a1d1dd5-4557-442f-8758-00ddaad0d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer.encode_plus(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac15bbab-2ad5-4edc-bb71-5d9979c45cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "          6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "          5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "          6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "          5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "         10822,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2047807b-32b7-4a0b-aac8-dd3378d187c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.vstack((token['input_ids'],token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86645fa6-12cc-48dd-969d-e11dd25c1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_type_ids = torch.vstack((token['token_type_ids'],token['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67a0b9e4-f3d4-4ee6-974d-0f09a4f7967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:01<00:00]\n"
     ]
    }
   ],
   "source": [
    "entities = inference_model(tokens['input_ids'], tokens['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9db7f811-0fac-4f5b-9928-8fc69a754748",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-bb9cb10b6a2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "entities = entities[0].cpu().numpy()\n",
    "input_ids = tokens[\"input_ids\"].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea0df7-3c92-4079-854f-672faabda6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n",
    "labels_idx = score.argmax(axis=-1)\n",
    "\n",
    "token_level_answer = []\n",
    "for idx, label_idx in enumerate(labels_idx):\n",
    "    # NOTE Append every answer even though the `entity` is in `ignore_labels`\n",
    "    token_level_answer += [\n",
    "        {\n",
    "            \"word\": tokenizer.convert_ids_to_tokens(int(input_ids[idx])),\n",
    "            \"score\": score[idx][label_idx].item(),\n",
    "            \"entity\": model.config.id2label[label_idx],\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a2590-3ae0-4008-8699-a76069cee82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4558a63-be2d-4619-affb-ca5f25405071",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_labels=[\"O\"]\n",
    "ignore_special_tokens=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b55a34-e27d-4e85-8b77-729e521860aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [FIX] Now let's change it to word-level NER\n",
    "word_idx = 0\n",
    "word_level_answer = []\n",
    "\n",
    "# NOTE: Might not be safe. BERT, ELECTRA etc. won't make issues.\n",
    "if ignore_special_tokens:\n",
    "    words = words[1:-1]\n",
    "    tokens_mask = tokens_mask[1:-1]\n",
    "    token_level_answer = token_level_answer[1:-1]\n",
    "\n",
    "for mask, ans in zip(tokens_mask, token_level_answer):\n",
    "    if mask == 1:\n",
    "        ans[\"word\"] = words[word_idx]\n",
    "        word_idx += 1\n",
    "        if ans[\"entity\"] not in ignore_labels:\n",
    "            word_level_answer.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769b7a1-a332-4d53-a9d7-57f25740abfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d11299e-efd8-4c44-b38a-b0fc4e1251cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '2009년', 'score': 0.9996150135993958, 'entity': 'DAT-B'},\n",
       " {'word': 'FC서울을', 'score': 0.9340113997459412, 'entity': 'ORG-B'},\n",
       " {'word': '떠나', 'score': 0.5750519037246704, 'entity': 'PER-B'},\n",
       " {'word': '잉글랜드', 'score': 0.9994958639144897, 'entity': 'ORG-B'},\n",
       " {'word': '프리미어리그', 'score': 0.9615978002548218, 'entity': 'ORG-I'}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_level_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bd73eea1-5ad0-4fe4-80e7-f43ba82399c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '2009년', 'score': 0.9996205568313599, 'entity': 'DAT-B'},\n",
       " {'word': '7월', 'score': 0.9350952506065369, 'entity': 'DAT-I'},\n",
       " {'word': 'FC서울을', 'score': 0.9994584321975708, 'entity': 'ORG-B'},\n",
       " {'word': '잉글랜드', 'score': 0.9983227849006653, 'entity': 'LOC-B'},\n",
       " {'word': '프리미어리그', 'score': 0.9989780187606812, 'entity': 'ORG-B'},\n",
       " {'word': '볼턴', 'score': 0.9300729632377625, 'entity': 'ORG-B'},\n",
       " {'word': '원더러스로', 'score': 0.999349057674408, 'entity': 'ORG-I'},\n",
       " {'word': '이청용은', 'score': 0.9994910359382629, 'entity': 'PER-B'},\n",
       " {'word': '크리스탈', 'score': 0.9994640350341797, 'entity': 'ORG-B'},\n",
       " {'word': '팰리스와', 'score': 0.9991778135299683, 'entity': 'ORG-I'},\n",
       " {'word': '독일', 'score': 0.9977171421051025, 'entity': 'LOC-B'},\n",
       " {'word': '분데스리가2', 'score': 0.9814788699150085, 'entity': 'ORG-B'},\n",
       " {'word': 'VfL', 'score': 0.8719519376754761, 'entity': 'ORG-B'},\n",
       " {'word': '보훔을', 'score': 0.9938763380050659, 'entity': 'ORG-I'},\n",
       " {'word': '지난', 'score': 0.9963383078575134, 'entity': 'DAT-B'},\n",
       " {'word': '3월', 'score': 0.9909418225288391, 'entity': 'DAT-I'},\n",
       " {'word': 'K리그로', 'score': 0.9995406270027161, 'entity': 'ORG-B'},\n",
       " {'word': '서울이', 'score': 0.9918311238288879, 'entity': 'ORG-B'},\n",
       " {'word': '울산이었다', 'score': 0.9994460940361023, 'entity': 'ORG-B'}]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_level_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e23d97-0694-49a9-be9a-5ddf50978562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f6839-fd02-4a5d-8ad3-5899e6328b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
