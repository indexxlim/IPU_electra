{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b6829e-91ef-4eb6-8184-20b6ae9e9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea51584-30da-49d7-8b15-91bc7c016d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import ElectraTokenizerFast, ElectraForTokenClassification\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"monologg/koelectra-small-finetuned-naver-ner\")\n",
    "model = ElectraForTokenClassification.from_pretrained(\"monologg/koelectra-small-finetuned-naver-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ffe60-f4d3-48af-95ce-299423547430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a4ead-473f-4c21-87e3-ddb0e3c3fdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b47bfb4-072c-4bd7-ace6-3d3fb396ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    BasicTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    Pipeline\n",
    ")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def custom_encode_plus(sentence,\n",
    "                       tokenizer,\n",
    "                       return_tensors=None):\n",
    "    # {'input_ids': [2, 10841, 10966, 10832, 10541, 21509, 27660, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "    words = sentence.split()\n",
    "\n",
    "    tokens = []\n",
    "    tokens_mask = []\n",
    "\n",
    "    for word in words:\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [tokenizer.unk_token]  # For handling the bad-encoded word\n",
    "        tokens.extend(word_tokens)\n",
    "        tokens_mask.extend([1] + [0] * (len(word_tokens) - 1))\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    len_ids = len(ids)\n",
    "    total_len = len_ids + tokenizer.num_special_tokens_to_add()\n",
    "    if tokenizer.model_max_length and total_len > tokenizer.model_max_length:\n",
    "        ids, _, _ = tokenizer.truncate_sequences(\n",
    "            ids,\n",
    "            pair_ids=None,\n",
    "            num_tokens_to_remove=total_len - tokenizer.model_max_length,\n",
    "            truncation_strategy=\"longest_first\",\n",
    "            stride=0,\n",
    "        )\n",
    "\n",
    "    sequence = tokenizer.build_inputs_with_special_tokens(ids)\n",
    "    token_type_ids = tokenizer.create_token_type_ids_from_sequences(ids)\n",
    "    # HARD-CODED: As I know, most of the transformers architecture will be `[CLS] + text + [SEP]``\n",
    "    #             Only way to safely cover all the cases is to integrate `token mask builder` in internal library.\n",
    "    tokens_mask = [1] + tokens_mask + [1]\n",
    "    words = [tokenizer.cls_token] + words + [tokenizer.sep_token]\n",
    "\n",
    "    encoded_inputs = {}\n",
    "    encoded_inputs[\"input_ids\"] = sequence\n",
    "    encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "    encoded_inputs[\"input_ids\"] = torch.tensor([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "    if \"token_type_ids\" in encoded_inputs:\n",
    "        encoded_inputs[\"token_type_ids\"] = torch.tensor([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "    if \"attention_mask\" in encoded_inputs:\n",
    "        encoded_inputs[\"attention_mask\"] = torch.tensor([encoded_inputs[\"attention_mask\"]])\n",
    "\n",
    "    elif return_tensors is not None:\n",
    "        logger.warning(\n",
    "            \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                return_tensors\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return encoded_inputs, words, tokens_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e669c50-37c3-45ea-b7d6-4c515a336776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46470fcf-6f56-4667-ae80-0640dc1bb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"2009년 7월 FC서울을 떠나 잉글랜드 프리미어리그 볼턴 원더러스로 이적한 이청용은 크리스탈 팰리스와 독일 분데스리가2 VfL 보훔을 거쳐 지난 3월 K리그로 컴백했다. 행선지는 서울이 아닌 울산이었다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9d4cb2-8894-4813-85da-874a424a1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, words, tokens_mask = custom_encode_plus(sentence, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a864ee-e5bd-4b85-93bf-5bd168b83361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "           6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "           5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "           6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "           5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "          10822,     3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee067c3-5ef5-43ab-be56-46f2b15f3c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "           6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "           5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "           6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "           5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "          10822,     3]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ce6798-c884-4044-97b9-66fada088c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6daa7cec-5885-4596-a85e-bbc1589c880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode_plus(sentence,\n",
    "    padding='longest',\n",
    "    max_length=512,\n",
    "    truncation=True, \n",
    "    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147f7bb-10ff-4a7c-9278-7cc44f111ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83598a-2352-4359-a72f-f52c99fce178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "882bfdc2-50ff-48b5-8d88-0ca2c93edbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e7dc78-54eb-44e4-819d-e0de6a7394f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = entities[0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c0ffab-de40-4ab7-a23f-ddb8f68ee72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.exp(result) / np.exp(result).sum(-1, keepdims=True)\n",
    "labels_idx = score.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd81ebf-b80c-4e06-aa32-48d414e1dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_level_answer = []\n",
    "input_ids = tokens[\"input_ids\"].numpy()[0]\n",
    "\n",
    "for idx, label_idx in enumerate(labels_idx):\n",
    "    # NOTE Append every answer even though the `entity` is in `ignore_labels`\n",
    "    token_level_answer += [\n",
    "        {\n",
    "            \"word\": tokenizer.convert_ids_to_tokens(int(input_ids[idx])),\n",
    "            \"score\": score[idx][label_idx].item(),\n",
    "            \"entity\": model.config.id2label[label_idx],\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d5ba3-94de-4ea0-b8da-cc26c03f1b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07e8d3-632c-475b-99d3-d1f7e7f68ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b845847-e184-4dc4-9f3d-cf3e7d04ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f083b260-6e8f-4238-b413-44358c61a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/naver-ner/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ad1d4c-69e4-40bf-9d04-c5d29bda686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, sep='\\t', names = ['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03646c58-d03b-406b-bf09-b80b7221402f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56575a-cab8-48e9-afa3-ce180f880e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08784159-2c62-4ceb-8bbf-2beaddd76958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a75d19-15f0-45b5-a4dc-a939271aad6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1381f-b6dd-454e-b21a-4eb396672022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc94306a-e37b-4e3d-9036-b58ee4f77750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data = pd.read_csv(data_path, sep='\\t', names = ['text', 'label'])\n",
    "        \n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        for i in range(len(data)):\n",
    "            self.texts.append(data.iloc[i].text)\n",
    "            self.labels = self.labels2id(data.iloc[i].label.split())\n",
    "\n",
    "                \n",
    "    def labels2id(self, labels):\n",
    "        labels_lst = [\"O\",\n",
    "                \"PER-B\", \"PER-I\", \"FLD-B\", \"FLD-I\", \"AFW-B\", \"AFW-I\", \"ORG-B\", \"ORG-I\",\n",
    "                \"LOC-B\", \"LOC-I\", \"CVL-B\", \"CVL-I\", \"DAT-B\", \"DAT-I\", \"TIM-B\", \"TIM-I\",\n",
    "                \"NUM-B\", \"NUM-I\", \"EVT-B\", \"EVT-I\", \"ANM-B\", \"ANM-I\", \"PLT-B\", \"PLT-I\",\n",
    "                \"MAT-B\", \"MAT-I\", \"TRM-B\", \"TRM-I\"]\n",
    "        labels_dict = {label:i for i, label in enumerate(labels_lst)}\n",
    "        id_value=[]\n",
    "        try:\n",
    "            for label in labels:\n",
    "                id_value.append(labels_dict[label])\n",
    "                print(id_value)\n",
    "        except:\n",
    "            raise Exception(f'Not in NER labels : {label}')\n",
    "        return id_value\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'text':self.texts[index], 'label':self.labels[index]}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9920980c-8c4a-44af-b32f-6f63708f5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NERCollator:\n",
    "    def __init__(self, tokenizer, mapping=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = 'text'\n",
    "        self.label = 'label'    \n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.text not in batch[0] or self.label not in batch[0]:\n",
    "            raise Exception(\"Error: Undefined data keys\")\n",
    "\n",
    "        sentence = [item[self.text]+self.tokenizer.eos_token for item in batch]\n",
    "\n",
    "        source_batch = self.tokenizer.batch_encode_plus(sentence,\n",
    "                    padding='longest',\n",
    "                    max_length=512,\n",
    "                    truncation=True, \n",
    "                    return_tensors='pt')\n",
    "       \n",
    "\n",
    "            \n",
    "        return {'input_ids':source_batch.input_ids,\n",
    "                 'attention_mask':source_batch.attention_mask,\n",
    "                 'token_type_ids':source_batch.token_type_ids,\n",
    "                 'labels': labels,\n",
    "                 }\n",
    "    \n",
    "    def custom_encode_plus(self, sentence, tokenizer, return_tensors=None):\n",
    "\n",
    "        words = sentence.split()\n",
    "\n",
    "        tokens = []\n",
    "        tokens_mask = []\n",
    "\n",
    "        for word in words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [tokenizer.unk_token]  # For handling the bad-encoded word\n",
    "            tokens.extend(word_tokens)\n",
    "            tokens_mask.extend([1] + [0] * (len(word_tokens) - 1))\n",
    "\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        len_ids = len(ids)\n",
    "        total_len = len_ids + tokenizer.num_special_tokens_to_add()\n",
    "        if tokenizer.model_max_length and total_len > tokenizer.model_max_length:\n",
    "            ids, _, _ = tokenizer.truncate_sequences(\n",
    "                ids,\n",
    "                pair_ids=None,\n",
    "                num_tokens_to_remove=total_len - tokenizer.model_max_length,\n",
    "                truncation_strategy=\"longest_first\",\n",
    "                stride=0,\n",
    "            )\n",
    "\n",
    "        sequence = tokenizer.build_inputs_with_special_tokens(ids)\n",
    "        token_type_ids = tokenizer.create_token_type_ids_from_sequences(ids)\n",
    "        # HARD-CODED: As I know, most of the transformers architecture will be `[CLS] + text + [SEP]``\n",
    "        #             Only way to safely cover all the cases is to integrate `token mask builder` in internal library.\n",
    "        tokens_mask = [1] + tokens_mask + [1]\n",
    "        words = [tokenizer.cls_token] + words + [tokenizer.sep_token]\n",
    "\n",
    "        encoded_inputs = {}\n",
    "        encoded_inputs[\"input_ids\"] = sequence\n",
    "        encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "\n",
    "        encoded_inputs[\"input_ids\"] = torch.tensor([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "        if \"token_type_ids\" in encoded_inputs:\n",
    "            encoded_inputs[\"token_type_ids\"] = torch.tensor([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "        if \"attention_mask\" in encoded_inputs:\n",
    "            encoded_inputs[\"attention_mask\"] = torch.tensor([encoded_inputs[\"attention_mask\"]])\n",
    "\n",
    "        elif return_tensors is not None:\n",
    "            logger.warning(\n",
    "                \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                    return_tensors\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return encoded_inputs, words, tokens_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3828803b-815f-486a-b689-5adee97fc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2id(labels):\n",
    "    labels_lst = [\"O\",\n",
    "            \"PER-B\", \"PER-I\", \"FLD-B\", \"FLD-I\", \"AFW-B\", \"AFW-I\", \"ORG-B\", \"ORG-I\",\n",
    "            \"LOC-B\", \"LOC-I\", \"CVL-B\", \"CVL-I\", \"DAT-B\", \"DAT-I\", \"TIM-B\", \"TIM-I\",\n",
    "            \"NUM-B\", \"NUM-I\", \"EVT-B\", \"EVT-I\", \"ANM-B\", \"ANM-I\", \"PLT-B\", \"PLT-I\",\n",
    "            \"MAT-B\", \"MAT-I\", \"TRM-B\", \"TRM-I\"]\n",
    "    labels_dict = {label:i for i, label in enumerate(labels_lst)}\n",
    "    id_value=[]\n",
    "    try:\n",
    "        for label in labels:\n",
    "            id_value.append(labels_dict[label])\n",
    "    except:\n",
    "        raise Exception(f'Not in NER labels : {label}')\n",
    "    return id_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b619850c-d119-43d6-8a8e-ddff5aca762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path, sep='\\t', names = ['text', 'label'])\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "for i in range(len(data)):\n",
    "    texts.append(data.iloc[i].text)\n",
    "    labels.append(labels2id(data.iloc[i].label.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031f3f1-1fa0-4b1a-81f1-dc793476d81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f9b98-1fcd-4634-a03b-71d587a80bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec73b045-eff5-4612-bc30-89bfc947a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, words, tokens_mask = custom_encode_plus(sentence, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "131299ac-895f-4115-ab68-12c3e0230db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89bd0fc1-4891-4afd-a4a3-c159b5fc2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d497840-bc64-441c-962e-052fbc3949e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ElectraTokenizer, ElectraForTokenClassification\n",
    "# import torch\n",
    "\n",
    "# tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "# model = ElectraForTokenClassification.from_pretrained('google/electra-small-discriminator')\n",
    "\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# labels = torch.tensor([1] * inputs[\"input_ids\"].size(1)).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)\n",
    "# loss, scores = outputs[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8394ad-487e-4857-8515-343d02adeabd",
   "metadata": {},
   "source": [
    "# IPU model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59f47160-43bc-46eb-a9f7-31fdf650dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f68bb9e-28d5-4f98-918f-cea8ada5948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline_electra import PipelinedElectraForTokenClassification\n",
    "from easydict import EasyDict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "911cb723-ae59-4745-8a0a-1d8f0bae27cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'finetune/squad_configurations.yaml'\n",
    "config = EasyDict(yaml.load(open(config_file).read(), Loader=yaml.Loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf9c24fc-2a9e-46dc-bd3e-44105dfbf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ipu_config = {\n",
    "    \"layers_per_ipu\": config.train_config.train_layers_per_ipu,\n",
    "    \"recompute_checkpoint_every_layer\": config.train_config.train_recompute_checkpoint_every_layer,\n",
    "    \"embedding_serialization_factor\": config.train_config.train_embedding_serialization_factor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a639f64a-48f2-4e91-90c1-5c7a76f33cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelinedElectraForTokenClassification.from_pretrained_transformers(\"monologg/koelectra-small-finetuned-naver-ner\", train_ipu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf389f72-b353-42e4-ab48-8fbe9825ffcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b5f3d80d-ae06-4b4c-852e-52b230e7d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "from finetune.run_squad_ipu import ipu_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b91790d9-3108-4de3-b001-5964d3d94363",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_micro_batch_size = 1\n",
    "valid_replication_factor = 1\n",
    "valid_global_batch_size = valid_micro_batch_size * valid_replication_factor\n",
    "valid_device_iterations = 1\n",
    "valid_samples_per_iteration = valid_global_batch_size * valid_device_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "190c64c9-c203-481c-8034-1b3fa6dfed91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_samples_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f237f996-fbcf-4feb-adea-c8bda9f61d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_opts = ipu_options(1, valid_replication_factor, valid_device_iterations, train_option=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "afcd4feb-355d-42d6-a554-c2fb634516a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = poptorch.inferenceModel(model, val_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8a1d1dd5-4557-442f-8758-00ddaad0d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token = tokenizer.encode_plus(sentence, return_tensors='pt', return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f721b3-81ee-4ef6-8ae7-b95df800e2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3f3e629d-4234-49e5-aac0-3efcac89c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sentence.split()\n",
    "split_tokens = tokenizer.encode_plus(word_list, is_split_into_words=True, return_tensors='pt', return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1f1a4a2a-29c1-4f7c-8efb-ac598b7e2ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_offset_mapping = split_tokens['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31809ebd-ca9e-4d2e-b41f-fede0b1e02d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2047807b-32b7-4a0b-aac8-dd3378d187c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = torch.vstack((token['input_ids'],token['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "86645fa6-12cc-48dd-969d-e11dd25c1c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_type_ids = torch.vstack((token['token_type_ids'],token['token_type_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "85063c74-4d41-44b3-a43f-2d3a9afa632d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 11909, 11243, 16810, 15953,  5703, 12584, 17085, 30548,  2355,\n",
       "          6134,  3658, 21851, 11184, 15334,  5696,  3777,  5736, 19686, 26772,\n",
       "          5129, 10856,  5813, 11276,  2428, 17804, 10731,  5920,    58,  5988,\n",
       "          6240,  2348,  7462,  5703, 12103, 10563, 11207, 21444,  5699,  4675,\n",
       "          5821, 10542,    18,  5386,  5843, 10639, 10602,  5706, 10940, 13125,\n",
       "         10822,     3]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "67a0b9e4-f3d4-4ee6-974d-0f09a4f7967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|██████████| 100/100 [00:02<00:00]\n"
     ]
    }
   ],
   "source": [
    "entities = inference_model(split_tokens['input_ids'], split_tokens['token_type_ids'])#, split_tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce72db1-4537-426c-8c18-da9b905993bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71332a0f-3ecd-4049-8675-38e154169c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9db7f811-0fac-4f5b-9928-8fc69a754748",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_labels=[\"O\"]\n",
    "ignore_special_tokens=True\n",
    "\n",
    "word_offset_mapping = word_offset_mapping[0].cpu().numpy()\n",
    "entities = entities[0].cpu().numpy()\n",
    "input_ids = split_tokens[\"input_ids\"].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "9ade88a7-9c76-46f5-8e87-64a1e402d998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 5],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [2, 4],\n",
       "       [4, 5],\n",
       "       [0, 2],\n",
       "       [0, 4],\n",
       "       [0, 6],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 5],\n",
       "       [0, 2],\n",
       "       [2, 3],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [0, 4],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 4],\n",
       "       [0, 2],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 5],\n",
       "       [5, 6],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [3, 4],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [4, 5],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [0, 2],\n",
       "       [2, 3],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [2, 5],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14b8c8-4f48-4c09-ac98-523c5179e351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "39ea0df7-3c92-4079-854f-672faabda6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n",
    "labels_idx = score.argmax(axis=-1)\n",
    "\n",
    "token_level_answer = []\n",
    "for idx, label_idx in enumerate(labels_idx):\n",
    "    if model.config.id2label[label_idx] in ignore_labels:\n",
    "        token_answer = []\n",
    "        continue\n",
    "    \n",
    "    elif word_offset_mapping[idx][0] == 0:\n",
    "        if token_answer:\n",
    "            token_answer[\"word\"] = tokenizer.decode(token_answer[\"word\"])\n",
    "            token_level_answer.append(token_answer)\n",
    "        token_answer = {\n",
    "                \"word\": [int(input_ids[idx])],\n",
    "                \"score\": score[idx][label_idx].item(),\n",
    "                \"entity\": model.config.id2label[label_idx],\n",
    "        }\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        token_answer['word'].append(int(input_ids[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f13daad8-415c-4413-9adf-cb2ce96fd323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '2009년', 'score': 0.9996205568313599, 'entity': 'DAT-B'},\n",
       " {'word': '7월', 'score': 0.9350952506065369, 'entity': 'DAT-I'},\n",
       " {'word': '잉글랜드', 'score': 0.9983227849006653, 'entity': 'LOC-B'},\n",
       " {'word': '프리미어리그', 'score': 0.9989780187606812, 'entity': 'ORG-B'},\n",
       " {'word': '볼턴', 'score': 0.9300729632377625, 'entity': 'ORG-B'},\n",
       " {'word': '이청용은', 'score': 0.9994910359382629, 'entity': 'PER-B'},\n",
       " {'word': '크리스탈', 'score': 0.9994640350341797, 'entity': 'ORG-B'},\n",
       " {'word': '독일', 'score': 0.9977171421051025, 'entity': 'LOC-B'},\n",
       " {'word': '분데스리가2', 'score': 0.9814788699150085, 'entity': 'ORG-B'},\n",
       " {'word': 'VfL', 'score': 0.8719519376754761, 'entity': 'ORG-B'},\n",
       " {'word': '지난', 'score': 0.9963383078575134, 'entity': 'DAT-B'},\n",
       " {'word': '3월', 'score': 0.9909418225288391, 'entity': 'DAT-I'}]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_level_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4d6f475f-7df4-48aa-8d27-40e7bf957076",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i, word_offset in enumerate(word_offset_mapping):\n",
    "    if (word_offset[1] - word_offset[0]) == 0:\n",
    "        token = []\n",
    "        continue\n",
    "    elif word_offset[0] == 0:\n",
    "        if token:\n",
    "            tokens.append(token)\n",
    "        token = []\n",
    "        token.append(split_tokens['input_ids'][0][i])\n",
    "    else:\n",
    "        token.append(split_tokens['input_ids'][0][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5a48e2da-a8bd-48a5-a0ae-7fe020072ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2009년',\n",
       " '7월',\n",
       " 'FC서울을',\n",
       " '떠나',\n",
       " '잉글랜드',\n",
       " '프리미어리그',\n",
       " '볼턴',\n",
       " '원더러스로',\n",
       " '이적한',\n",
       " '이청용은',\n",
       " '크리스탈',\n",
       " '팰리스와',\n",
       " '독일',\n",
       " '분데스리가2',\n",
       " 'VfL',\n",
       " '보훔을',\n",
       " '거쳐',\n",
       " '지난',\n",
       " '3월',\n",
       " 'K리그로',\n",
       " '컴백했다.',\n",
       " '행선지는',\n",
       " '서울이',\n",
       " '아닌',\n",
       " '울산이었다']"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "12a3a2ec-8bb6-48fd-b37b-f5f3c1ff847c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 5],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [2, 4],\n",
       "       [4, 5],\n",
       "       [0, 2],\n",
       "       [0, 4],\n",
       "       [0, 6],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 5],\n",
       "       [0, 2],\n",
       "       [2, 3],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [0, 4],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 4],\n",
       "       [0, 2],\n",
       "       [0, 1],\n",
       "       [1, 3],\n",
       "       [3, 5],\n",
       "       [5, 6],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [3, 4],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [4, 5],\n",
       "       [0, 1],\n",
       "       [1, 2],\n",
       "       [2, 4],\n",
       "       [0, 2],\n",
       "       [2, 3],\n",
       "       [0, 2],\n",
       "       [0, 2],\n",
       "       [2, 5],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_offset_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "29fd9399-8afe-4d1e-a3c2-bd7c5f6c5226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009년'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a57cc14c-bf7a-4139-b45d-2fc851f9da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009년\n",
      "7월\n",
      "FC서울을\n",
      "떠나\n",
      "잉글랜드\n",
      "프리미어리그\n",
      "볼턴\n",
      "원더러스로\n",
      "이적한\n",
      "이청용은\n",
      "크리스탈\n",
      "팰리스와\n",
      "독일\n",
      "분데스리가2\n",
      "VfL\n",
      "보훔을\n",
      "거쳐\n",
      "지난\n",
      "3월\n",
      "K리그로\n",
      "컴백했다.\n",
      "행선지는\n",
      "서울이\n",
      "아닌\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a80acf3-f702-4c43-b4e2-73f88707938c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedaa82c-46de-40e3-84d4-7df45f8fdea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "26757fb4-3f73-4c8f-b4b6-408967e48d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cur_offset_mapping = offset_mapping[0]\n",
    "for i, offset in enumerate(cur_offset_mapping[1:-1]):\n",
    "    #print(offset)\n",
    "    if (cur_offset_mapping[i+1][0] - offset[1]) > 0:\n",
    "        continue\n",
    "    else:\n",
    "        print(token_level_answer[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4037bf49-7b77-4558-944f-0c157ce274ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '7월', 'score': 0.9350952506065369, 'entity': 'DAT-I'},\n",
       " {'word': 'FC', 'score': 0.9994584321975708, 'entity': 'ORG-B'},\n",
       " {'word': '##서울', 'score': 0.9809041023254395, 'entity': 'ORG-I'},\n",
       " {'word': '##을', 'score': 0.9081450700759888, 'entity': 'O'},\n",
       " {'word': '떠나', 'score': 0.9999575614929199, 'entity': 'O'},\n",
       " {'word': '잉글랜드', 'score': 0.9983227849006653, 'entity': 'LOC-B'},\n",
       " {'word': '프리미어리그', 'score': 0.9989780187606812, 'entity': 'ORG-B'},\n",
       " {'word': '볼', 'score': 0.9300729632377625, 'entity': 'ORG-B'},\n",
       " {'word': '##턴', 'score': 0.9710206389427185, 'entity': 'ORG-I'},\n",
       " {'word': '원', 'score': 0.999349057674408, 'entity': 'ORG-I'},\n",
       " {'word': '##더러', 'score': 0.9925868511199951, 'entity': 'ORG-I'},\n",
       " {'word': '##스로', 'score': 0.9959636926651001, 'entity': 'ORG-I'},\n",
       " {'word': '이적', 'score': 0.9999213814735413, 'entity': 'O'},\n",
       " {'word': '##한', 'score': 0.9999334216117859, 'entity': 'O'},\n",
       " {'word': '이', 'score': 0.9994910359382629, 'entity': 'PER-B'},\n",
       " {'word': '##청', 'score': 0.7056900262832642, 'entity': 'PER-B'},\n",
       " {'word': '##용은', 'score': 0.5724880695343018, 'entity': 'PER-B'},\n",
       " {'word': '크리스탈', 'score': 0.9994640350341797, 'entity': 'ORG-B'},\n",
       " {'word': '팰', 'score': 0.9991778135299683, 'entity': 'ORG-I'},\n",
       " {'word': '##리스', 'score': 0.9978461265563965, 'entity': 'ORG-I'},\n",
       " {'word': '##와', 'score': 0.9522739052772522, 'entity': 'O'},\n",
       " {'word': '독일', 'score': 0.9977171421051025, 'entity': 'LOC-B'},\n",
       " {'word': '분', 'score': 0.9814788699150085, 'entity': 'ORG-B'},\n",
       " {'word': '##데스', 'score': 0.8572928309440613, 'entity': 'ORG-B'},\n",
       " {'word': '##리가', 'score': 0.9942399859428406, 'entity': 'ORG-I'},\n",
       " {'word': '##2', 'score': 0.9120205640792847, 'entity': 'ORG-I'},\n",
       " {'word': 'V', 'score': 0.8719519376754761, 'entity': 'ORG-B'},\n",
       " {'word': '##f', 'score': 0.7203494310379028, 'entity': 'ORG-I'},\n",
       " {'word': '##L', 'score': 0.9656755328178406, 'entity': 'ORG-I'},\n",
       " {'word': '보', 'score': 0.9938763380050659, 'entity': 'ORG-I'},\n",
       " {'word': '##훔', 'score': 0.9126798510551453, 'entity': 'ORG-I'},\n",
       " {'word': '##을', 'score': 0.6827937960624695, 'entity': 'O'},\n",
       " {'word': '거쳐', 'score': 0.9999727606773376, 'entity': 'O'},\n",
       " {'word': '지난', 'score': 0.9963383078575134, 'entity': 'DAT-B'},\n",
       " {'word': '3월', 'score': 0.9909418225288391, 'entity': 'DAT-I'},\n",
       " {'word': 'K리그', 'score': 0.9995406270027161, 'entity': 'ORG-B'},\n",
       " {'word': '##로', 'score': 0.9990398287773132, 'entity': 'O'},\n",
       " {'word': '컴', 'score': 0.9999098181724548, 'entity': 'O'},\n",
       " {'word': '##백', 'score': 0.998866856098175, 'entity': 'O'},\n",
       " {'word': '##했다', 'score': 0.9999646544456482, 'entity': 'O'},\n",
       " {'word': '.', 'score': 0.9999403357505798, 'entity': 'O'},\n",
       " {'word': '행', 'score': 0.9993055462837219, 'entity': 'O'},\n",
       " {'word': '##선', 'score': 0.9969778656959534, 'entity': 'O'},\n",
       " {'word': '##지는', 'score': 0.9959809184074402, 'entity': 'O'},\n",
       " {'word': '서울', 'score': 0.9918311238288879, 'entity': 'ORG-B'},\n",
       " {'word': '##이', 'score': 0.9753754138946533, 'entity': 'O'},\n",
       " {'word': '아닌', 'score': 0.9999187588691711, 'entity': 'O'},\n",
       " {'word': '울산', 'score': 0.9994460940361023, 'entity': 'ORG-B'}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_level_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e4558a63-be2d-4619-affb-ca5f25405071",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_labels=[\"O\"]\n",
    "ignore_special_tokens=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d6b55a34-e27d-4e85-8b77-729e521860aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [FIX] Now let's change it to word-level NER\n",
    "word_idx = 0\n",
    "word_level_answer = []\n",
    "\n",
    "# NOTE: Might not be safe. BERT, ELECTRA etc. won't make issues.\n",
    "if ignore_special_tokens:\n",
    "    words = words[1:-1]\n",
    "    tokens_mask = tokens_mask[1:-1]\n",
    "    token_level_answer = token_level_answer[1:-1]\n",
    "\n",
    "for mask, ans in zip(tokens_mask, token_level_answer):\n",
    "    if mask == 1:\n",
    "        ans[\"word\"] = words[word_idx]\n",
    "        word_idx += 1\n",
    "        if ans[\"entity\"] not in ignore_labels:\n",
    "            word_level_answer.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73eea1-5ad0-4fe4-80e7-f43ba82399c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4e39f1ce-ddc2-4214-a29a-fce7a341addc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '2009년',\n",
       " '7월',\n",
       " 'FC서울을',\n",
       " '떠나',\n",
       " '잉글랜드',\n",
       " '프리미어리그',\n",
       " '볼턴',\n",
       " '원더러스로',\n",
       " '이적한',\n",
       " '이청용은',\n",
       " '크리스탈',\n",
       " '팰리스와',\n",
       " '독일',\n",
       " '분데스리가2',\n",
       " 'VfL',\n",
       " '보훔을',\n",
       " '거쳐',\n",
       " '지난',\n",
       " '3월',\n",
       " 'K리그로',\n",
       " '컴백했다.',\n",
       " '행선지는',\n",
       " '서울이',\n",
       " '아닌',\n",
       " '울산이었다',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e37fca02-250f-44a1-b9e1-570e8320c542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '2009년', 'score': 0.9996205568313599, 'entity': 'DAT-B'},\n",
       " {'word': '7월', 'score': 0.9350952506065369, 'entity': 'DAT-I'},\n",
       " {'word': 'FC', 'score': 0.9994584321975708, 'entity': 'ORG-B'},\n",
       " {'word': '##서울', 'score': 0.9809041023254395, 'entity': 'ORG-I'},\n",
       " {'word': '잉글랜드', 'score': 0.9983227849006653, 'entity': 'LOC-B'},\n",
       " {'word': '프리미어리그', 'score': 0.9989780187606812, 'entity': 'ORG-B'},\n",
       " {'word': '볼', 'score': 0.9300729632377625, 'entity': 'ORG-B'},\n",
       " {'word': '##턴', 'score': 0.9710206389427185, 'entity': 'ORG-I'},\n",
       " {'word': '원', 'score': 0.999349057674408, 'entity': 'ORG-I'},\n",
       " {'word': '##더러', 'score': 0.9925868511199951, 'entity': 'ORG-I'},\n",
       " {'word': '##스로', 'score': 0.9959636926651001, 'entity': 'ORG-I'},\n",
       " {'word': '이', 'score': 0.9994910359382629, 'entity': 'PER-B'},\n",
       " {'word': '##청', 'score': 0.7056900262832642, 'entity': 'PER-B'},\n",
       " {'word': '##용은', 'score': 0.5724880695343018, 'entity': 'PER-B'},\n",
       " {'word': '크리스탈', 'score': 0.9994640350341797, 'entity': 'ORG-B'},\n",
       " {'word': '팰', 'score': 0.9991778135299683, 'entity': 'ORG-I'},\n",
       " {'word': '##리스', 'score': 0.9978461265563965, 'entity': 'ORG-I'},\n",
       " {'word': '독일', 'score': 0.9977171421051025, 'entity': 'LOC-B'},\n",
       " {'word': '분', 'score': 0.9814788699150085, 'entity': 'ORG-B'},\n",
       " {'word': '##데스', 'score': 0.8572928309440613, 'entity': 'ORG-B'},\n",
       " {'word': '##리가', 'score': 0.9942399859428406, 'entity': 'ORG-I'},\n",
       " {'word': '##2', 'score': 0.9120205640792847, 'entity': 'ORG-I'},\n",
       " {'word': 'V', 'score': 0.8719519376754761, 'entity': 'ORG-B'},\n",
       " {'word': '##f', 'score': 0.7203494310379028, 'entity': 'ORG-I'},\n",
       " {'word': '##L', 'score': 0.9656755328178406, 'entity': 'ORG-I'},\n",
       " {'word': '보', 'score': 0.9938763380050659, 'entity': 'ORG-I'},\n",
       " {'word': '##훔', 'score': 0.9126798510551453, 'entity': 'ORG-I'},\n",
       " {'word': '지난', 'score': 0.9963383078575134, 'entity': 'DAT-B'},\n",
       " {'word': '3월', 'score': 0.9909418225288391, 'entity': 'DAT-I'},\n",
       " {'word': 'K리그', 'score': 0.9995406270027161, 'entity': 'ORG-B'},\n",
       " {'word': '서울', 'score': 0.9918311238288879, 'entity': 'ORG-B'},\n",
       " {'word': '울산', 'score': 0.9994460940361023, 'entity': 'ORG-B'}]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_level_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844f611-b246-4b09-8b75-4440e2c4f327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b445bb-411e-4999-a7b1-125736b2f3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
